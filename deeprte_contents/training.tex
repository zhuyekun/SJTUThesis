\section{Training}\label{sec:training}
In this section, we detail the training framework for our neural network model designed to approximate the Green's function in the Radiative Transfer Equation (RTE). Our approach leverages a delta function dataset, in which boundary conditions are approximated by Gaussian functions. This approach gives the model flexibility to handle boundary conditions not seen in training. This ability is similar to pre-training: it transfers useful knowledge from one task to others. The following subsections describe the construction of training dataset, the loss function based on the mean square error, and the evaluation methodology using relative error metrics.

\subsection{RTE dataset features}
\begin{table}[ht]
	\centering
	\begin{tabular}{lll}
		\toprule
		Features \& Shape                                 & Description                                                           \\
		\midrule
		\texttt{phase\_coords}: $[N_{\text{coords}},2d]$  & Phase coordinates $(\br, \bOmega)$                                    \\
		\texttt{boundary\_coords}: $[N_{\text{bc}},2d]$   & Boundary coordinates $(\br^{\prime}, \bOmega^{\prime})$               \\
		\texttt{boundary\_weights}: $[N_{\text{bc}}]$     & Boundary weights $\omega_i$                                           \\
		\texttt{position\_coords}: $[N_{\text{mesh}},2d]$ & Mesh points $(\br^{\text{mesh}})$                                     \\
		\texttt{velocity\_coords}: $[N_{\text{quad}},2d]$ & Angular quadrature points  $\bOmega^{*}$                              \\
		\texttt{velocity\_weights}: $[N_{\text{quad}}]$   & Angular quadrature weights $\omega_i$                                 \\
		\texttt{boundary}: $[N_{\text{bc}}]$              & Boundary $I(\br^{\prime}, \bOmega^{\prime})$                          \\
		\texttt{mu}: $[N_{\text{mesh}},2]$                & Cross section $\mus(\br^{\text{mesh}})$ and $\mut(\br^{\text{mesh}})$ \\
		\texttt{scattering\_kernel}: $[N_{\text{quad}}]$  & Scattering kernel $p(\bOmega, \bOmega^*)$                             \\
		\bottomrule
	\end{tabular}
	\caption{DeepRTE features. $N_\text{coords}$ denotes the number of phase-space coordinates, $N_\text{bc}$ represents the number of boundary coordinates, $N_\text{mesh}$ corresponds to the number of coefficient coordinates, $N_\text{quad}$ specifies the number of velocity quadrature points.}\label{tab:rte_features}
\end{table}

To construct training dataset, each training sample include the phase coordinates, the inflow boundary function $I_-$, scattering cross section $\mus$, total cross section $\mut$, the phase function $p$, and the intensity $I$. While these physical quantities are theoretically continuous functions, we approximate them through discrete sampling points in practice.
Table~\ref{tab:rte_features} summarizes the features included in each training sample.

% \textcolor{red}{Table~\ref{tab:rte_features} summarizes the features included in each training sample. Each sample consists of $N_\text{coords}$ phase-space coordinates, ${\{(\br_i, \bOmega_i)\}}_{i=1}^{N_\text{coords}}$, and the corresponding intensity values, ${\{I(\br_i, \bOmega_i)\}}_{i=1}^{N_\text{coords}}$.
% Additionally, the dataset includes $N_\text{bc}$ boundary coordinates, and the corresponding boundary weights.
% The boundary conditions are represented by the boundary intensity values, ${\{I(\br^{\prime}_i, \bOmega^{\prime}_i)\}}_{i=1}^{N_\text{bc}}$.
% The dataset also contains $N_\text{mesh}$ position coordinates, $N_\text{quad}$ velocity coordinates, and the corresponding velocity weights.
% The coefficient $\mu$ is represented by $N_\text{mesh}$ values, ${\{\mu(\br^{\text{mesh}}_i)\}}_{i=1}^{N_\text{mesh}}$, and the scattering kernel is represented by $N_\text{quad}$ values, ${\{p(\bOmega_i, \bOmega^*_i)\}}_{i=1}^{N_\text{quad}}$.(removed)}

\paragraph{Mesh-free sampling}
The training dataset is generated by a fully mesh-free sampling approach.
All feature counts $N_{\text{coords}}$, $N_{\text{bc}}$, $N_{\text{mesh}}$ and $N_{\text{quad}}$ are chosen independently.
This enables adjustments based on problem complexity and computational resources.

% \subsection{Pencil-beam problem}
\subsection{Delta function training dataset}
Due to the linearity of the RTE solution operator with respect to the inflow boundary condition,
any practical boundary condition can be approximated as a linear combination of delta-like functions. Therefore, by specializing our training dataset to approximate delta functions (e.g., using Gaussians), and leveraging the linearity of both the solution operator $\mathcal{A}$ and our DeepRTE approximation $\ANN$ with respect to $I_-$, a model accurately predicting inflow boundary conditions of the form~\eqref{eq:rte-greens-function} inherently generalizes to other boundary conditions.

\input{contents/error_estimate.tex}

In our implementation, we use the following Gaussian mollification kernel
\begin{equation}
	\zeta_\sigma(\br-\br', \bOmega-\bOmega') := \delta^{\sigma_{\br}}_{\{\br'\}}(\br)\delta^{\sigma_{\bOmega}}(\bOmega-\bOmega'),
\end{equation}
where
\begin{equation}\label{eq:delta_defination}
	\delta_{\{\br'\}}^{\sigma}(\br) = \frac{1}{\sigma \sqrt{\pi}} \exp\left( -\frac{{(\br-\br')}^2}{\sigma^2} \right), \quad
	\delta^{\sigma}(\bOmega-\bOmega') = \frac{1}{\sigma \sqrt{\pi}} \exp\left( -\frac{(\bOmega-\bOmega')^2}{\sigma^2} \right),
\end{equation}
where $(\br', \bOmega') \in \Gamma_-$.

\subsection{Loss and optimization}
The loss function is defined as the mean squared error (MSE) between the predicted intensity $I^{\text{NN}}_{\bm{\theta}}$ and the corresponding ground truth $I$:
\begin{equation}\label{eq:mse}
	\ell(I^\text{NN}_{\bm{\theta}}, I)= \frac{1}{N_\text{coords}}\sum_{i=1}^{N_{\text{coords}}} \left| I^{\text{NN}}_{\bm{\theta}}(\br_i,\bOmega_i) - I(\br_i, \bOmega_i) \right|^2,
\end{equation}
\begin{equation}\label{eq:loss}
	\mathcal{L}(\bm{\theta}) = \frac{1}{N}\sum_{n=1}^N \ell(I^\text{NN}_{\bm{\theta},n}, I_n),
\end{equation}
where $N_{\text{coords}}$ is the number of phase coordinates. However, to conserve computational resources during training, inspired by stochastic gradient descent principles, we sample only $N_\text{col}$ collocation points within the phase space at each gradient descent step:

\begin{equation}\label{eq:collocation_points} \ell(I^\text{NN}_{\bm{\theta}}, I)= \frac{1}{N_\text{col}}\sum_{i=1}^{N_{\text{col}}} \left| I^{\text{NN}}_{\bm{\theta}}(\br_i,\bOmega_i) - I(\br_i, \bOmega_i) \right|^2,
\end{equation}

In practice, training a neural network involves minimizing a loss function with respect to its parameters, $\bm{\theta}$. This optimization problem is formulated as:
\begin{equation}
	\min_{\bm{\theta}} \mathcal{L}(\bm{\theta}),
\end{equation}
and is typically solved using methods such as stochastic gradient descent (SGD) or advanced variants like Adam~\cite{kingma2017,rakhlin2012}.

\subsection{Accuracy and evaluation}
In practical applications, the density is a crucial and widely adopted metric for evaluation. The density is obtained by integrating the intensity $I(\br,\bOmega)$ with respect to $\bOmega$ over the $d$-dimensional unit sphere $\sS^{d-1}$, and is mathematically expressed as:
\begin{equation}\label{eq:density}
	\Phi(\br) = \frac{1}{S^{d-1}}\int_{\sS^{d-1}} I(\br, \bOmega) \diff{\bOmega},
\end{equation}

During the model's training phase, the target loss function is the mean squared error (MSE) of $I(\br,\bOmega)$ (see~\eqref{eq:loss}).
However, when evaluating the model's performance, the mean squared error is calculated based on the density.
Specifically, we define this MSE as:
\begin{equation}
	\text{MSE} = \frac{1}{N_{\text{mesh}}}\sum_{i=1}^{N_{\text{mesh}}} | \Phi^{\text{NN}}(\br_i) - \Phi(\br_i) |^2.
\end{equation}

Additionally, one can compute the relative error and we use the root mean square percentage error (RMSPE), defined as:
\begin{equation}
	\text{RMSPE} = \sqrt{\frac{\displaystyle\sum^{N_{\text{mesh}}}_{i=1} |\Phi^{\text{NN}}(\br_i) - \Phi(\br_i)|^2}{\displaystyle\sum^{N_{\text{mesh}}}_{i=1}|\Phi(\br_i)|^2}}\cdot 100\%.
\end{equation}
